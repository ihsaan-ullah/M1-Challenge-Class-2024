{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Kit - Relevance \n",
    "\n",
    "TODOs: \n",
    "- Add detailed description of the challenge\n",
    "- describe your data\n",
    "- describe how you will evaluate\n",
    "- provide instructions to the participants about what they should do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Setup\n",
    "***\n",
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB='google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # clone github repo\n",
    "    !git clone https://github.com/ihsaan-ullah/M1-Challenge-Class-2024.git\n",
    "\n",
    "    # move to the HEP starting kit folder\n",
    "    %cd M1-Challenge-Class-2024/Relevance/Starting_Kit/\n",
    "\n",
    "    !pip install -q --upgrade sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Directories\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./\"\n",
    "# Input data directory to read training data from\n",
    "input_dir = root_dir + \"sample_data/\"\n",
    "# Reference data directory to read test labels from\n",
    "reference_dir = root_dir + \"sample_data/\"\n",
    "# Output data directory to write predictions to\n",
    "output_dir = root_dir + \"sample_result_submission\"\n",
    "# Program directory\n",
    "program_dir = root_dir + \"ingestion_program\"\n",
    "# Score directory\n",
    "score_dir = root_dir + \"scoring_program\"\n",
    "# Directory to read submitted submissions from\n",
    "submission_dir = root_dir + \"sample_code_submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Add directories to path\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(input_dir)\n",
    "sys.path.append(reference_dir)\n",
    "sys.path.append(output_dir)\n",
    "sys.path.append(program_dir)\n",
    "sys.path.append(submission_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data\n",
    "***\n",
    "1. Load Data\n",
    "2. Preprocess data\n",
    "\n",
    "\n",
    "TODOS:\n",
    "- show data statistics\n",
    "\n",
    "### ⚠️ Note:\n",
    "The data used here is sample data is for demonstration only to get a view of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "      self.df = None\n",
    "\n",
    "      print(\"==========================================\")\n",
    "      print(\"Data\")\n",
    "      print(\"==========================================\")\n",
    "\n",
    "  def load_data(self):\n",
    "    \"\"\"\n",
    "      Loads data from csv file\n",
    "    \"\"\"\n",
    "    print(\"[*] Loading Data\")\n",
    "\n",
    "    # data file path\n",
    "    data_file = os.path.join(input_dir, 'relevance_sample_data.csv')\n",
    "    \n",
    "    # read data\n",
    "    self.df = pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "  def _text_to_dict(self, text):\n",
    "    \"\"\"\n",
    "    Converts a text string into a dictionary.\n",
    "\n",
    "    :param text: A string representation of a dictionary.\n",
    "    :return: A dictionary object if conversion is successful, otherwise {}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      return ast.literal_eval(text)\n",
    "    except:\n",
    "      return {}  # Return an empty dictionary in case of an error\n",
    "    \n",
    "  def _dict_to_paragraphs(self, dictionary):\n",
    "    \"\"\"\n",
    "    Converts a dictionary into a string of paragraphs.\n",
    "\n",
    "    :param dictionary: A dictionary.\n",
    "    :return: A string composed of paragraphs based on the dictionary's key-value pairs.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i, (k, v) in enumerate(dictionary.items()):\n",
    "        text += k.capitalize() + '\\n' + v + '\\n'\n",
    "    return text\n",
    "  \n",
    "  def transfrom_data(self):\n",
    "\n",
    "    print(\"[*] Transforming Data\")\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    self.df['most_relevant_dict'] = self.df['most_relevant'].apply(self._text_to_dict)\n",
    "    self.df['second_most_relevant_dict'] = self.df['second_most_relevant'].apply(self._text_to_dict)\n",
    "    self.df['second_least_relevant_dict'] = self.df['second_least_relevant'].apply(self._text_to_dict)\n",
    "    self.df['least_relevant_dict'] = self.df['least_relevant'].apply(self._text_to_dict)\n",
    "\n",
    "\n",
    "    # Convert from dictionary to text\n",
    "    self.df['most_relevant_text'] = self.df['most_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['second_most_relevant_text'] = self.df['second_most_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['second_least_relevant_text'] = self.df['second_least_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "    self.df['least_relevant_text'] = self.df['least_relevant_dict'].apply(self._dict_to_paragraphs)\n",
    "\n",
    "\n",
    "  def _get_embeddings(self, text1, text2):\n",
    "    \"\"\"\n",
    "    Generates embeddings for two texts.\n",
    "\n",
    "    :param text1: First text string.\n",
    "    :param text2: Second text string.\n",
    "    :return: Tuple of embeddings for text1 and text2.\n",
    "    \"\"\"\n",
    "    embedding1 = self.embeddings_model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = self.embeddings_model.encode(text2, convert_to_tensor=True)\n",
    "    return embedding1.cpu(), embedding2.cpu()\n",
    "  \n",
    "  def prepare_data(self):\n",
    "\n",
    "    print(\"[*] Prepare Data for Training\")\n",
    "    \n",
    "    model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.embeddings_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    # Create embeddings for each pair\n",
    "    self.df['most_relevant_embeddings'] = self.df.progress_apply(lambda row: self._get_embeddings(row['prompt'], row['most_relevant_text']), axis=1)\n",
    "    self.df['second_most_relevant_embeddings'] = self.df.progress_apply(lambda row: self._get_embeddings(row['prompt'], row['second_most_relevant_text']), axis=1)\n",
    "    self.df['second_least_relevant_embeddings'] = self.df.progress_apply(lambda row: self._get_embeddings(row['prompt'], row['second_least_relevant_text']), axis=1)\n",
    "    self.df['least_relevant_embeddings'] = self.df.progress_apply(lambda row: self._get_embeddings(row['prompt'], row['least_relevant_text']), axis=1)\n",
    "\n",
    "    # Label the Data\n",
    "    self.df['most_relevant_label'] = 3\n",
    "    self.df['second_most_relevant_label'] = 2\n",
    "    self.df['second_least_relevant_label'] = 1\n",
    "    self.df['least_relevant_label'] = 0\n",
    "\n",
    "    X = self.df['most_relevant_embeddings'].tolist() + self.df['second_most_relevant_embeddings'].tolist() + self.df['second_least_relevant_embeddings'].tolist() + self.df['least_relevant_embeddings'].tolist()\n",
    "    y = self.df['most_relevant_label'].tolist() + self.df['second_most_relevant_label'].tolist() + self.df['second_least_relevant_label'].tolist() + self.df['least_relevant_label'].tolist()\n",
    "\n",
    "    # Convert embeddings from tuples to concatenated arrays\n",
    "    X = [torch.abs(embeddings[0] - embeddings[1]).numpy() for embeddings in X]\n",
    "\n",
    "    # Shuffle X and y\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "    # train test split\n",
    "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "  def get_train_data(self):\n",
    "    return self.X_train, self.y_train\n",
    "  \n",
    "  def get_test_data(self):\n",
    "    return self.X_test, self.y_test\n",
    "  \n",
    "  def show_random_sample(self):\n",
    "    random_sample_index = np.random.randint(0, len(self.df))\n",
    "\n",
    "    print(\"Prompt:\\n\", self.df.iloc[random_sample_index]['prompt'], \"...\\n\")\n",
    "    print(\"Most Relevant Text:\\n\", self.df.iloc[random_sample_index]['most_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Second Most Relevant Text:\\n\", self.df.iloc[random_sample_index]['second_most_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Second Least Relevant Text:\\n\", self.df.iloc[random_sample_index]['second_least_relevant_text'][:300], \"...\\n\")\n",
    "    print(\"Least Relevant Text:\\n\", self.df.iloc[random_sample_index]['least_relevant_text'][:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Data\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Initilaize data\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading Data\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Transforming Data\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "data.transfrom_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Prepare Data for Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537a4b09b3064796b5fd159f9d9a9c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334819fb824b454e99361acfc4f2d6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666a1c3c159c4e4a9776bbacb3b10d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a579f7cd279942cd97378839390c43b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Write a systematic survey or overview about the cross-modal associations between visual and tactile experiences in abstract art, specifically focusing on the perception of texture and weight in relation to color presentation and real-life viewing. Investigate the potential differences in associations between natural colors, inverted colors, and black and white presentations, as well as the impact of viewing the paintings on a computer screen versus in a museum setting. ...\n",
      "\n",
      "Most Relevant Text:\n",
      " Title\n",
      "The Tactile Dimensions of Abstract Paintings: A Cross-Modal Study\n",
      "Abstract\n",
      "In our research, we tested for the existence of cross-modal visual and tactile associations in the experience of abstract art. Specifically, we measured the association of 60 abstract paintings with four couples of anto ...\n",
      "\n",
      "Second Most Relevant Text:\n",
      " Title\n",
      "The hue of shapes.\n",
      "Abstract\n",
      "This article presents an experimental study on the naturally biased association between shape and color. For each basic geometric shape studied, participants were asked to indicate the color perceived as most closely related to it, choosing from the Natural Color Sy ...\n",
      "\n",
      "Second Least Relevant Text:\n",
      " Title\n",
      "Indigenous and Traditional Visual Artistic Practices: Implications for Art Therapy Clinical Practice and Research\n",
      "Abstract\n",
      "In this paper, we present a review of research on the role of traditional and indigenous forms of visual artistic practice in promoting physical health and psychosocial we ...\n",
      "\n",
      "Least Relevant Text:\n",
      " Title\n",
      "Digital infrastructure for art historical research: thinking about user needs\n",
      "Abstract\n",
      "s for tracking resources. To begin with, chaining is a significant seeking activity in art historical research and it is usually carried out through, for example, books, articles, Digital Infrastructure for  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show random sample from data\n",
    "data.show_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train test data\n",
    "# X_train, y_train = data.get_train_data()\n",
    "# X_test, Y_test = data.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualize\n",
    "***\n",
    "TODOs:\n",
    "- visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualize():\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Visualize\")\n",
    "        print(\"==========================================\")\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_data(self):\n",
    "        print(\"Implement this function\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Visualize\n",
      "==========================================\n",
      "Implement this function\n"
     ]
    }
   ],
   "source": [
    "# Initilaize Visualize\n",
    "visualize = Visualize(data=data)\n",
    "visualize.visualize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Submission Model\n",
    "***\n",
    "We import a class named `Model` from the submission file (`model.py`). This `Model` class has the following methods:\n",
    "- `init`: initializes classifier\n",
    "- `fit`: gets train data and labels as input to train the classifier\n",
    "- `predict`: gets test data and outputs predictions made by the trained classifier\n",
    "\n",
    "\n",
    "In this example code, the `Model` class implements a Gradient Boosting Classifier model. You can find the code in `M1-Challenge-Class-2024/Relevance/Starting_Kit/sample_code_submission/model.py`. You can modify it the way you want, keeping the required class structure and functions there. More instructions are given inside the `model.py` file. If running in Collab, click the folder icon in the left sidebar to open the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Program\n",
    "***\n",
    "**`Ingestion program`** is responsible to run the submission of a participant on Codabench platform. **`Program`** is a simplified version of the **Ingestion Program** to show to participants how it runs a submission.\n",
    "1. Train a model on train data\n",
    "2. Predict using Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program():\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        # used to keep object of Model class to run the submission\n",
    "        self.model = None\n",
    "        # object of Data class used here to get the train and test sets\n",
    "        self.data = data\n",
    "\n",
    "        # results\n",
    "        self.results = []\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Program\")\n",
    "        print(\"==========================================\")\n",
    "    \n",
    "    def initialize_submission(self):\n",
    "        print(\"[*] Initializing Submmited Model\")\n",
    "        self.model = Model()\n",
    "\n",
    "    def fit_submission(self):\n",
    "        print(\"[*] Calling fit method of submitted model\")\n",
    "        X_train, y_train  = self.data.get_train_data()\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    def predict_submission(self):\n",
    "        print(\"[*] Calling predict method of submitted model\")\n",
    "      \n",
    "        X_test, _ = self.data.get_test_data()\n",
    "        self.y_test_hat = self.model.predict(X_test)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Program\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Intiialize Program\n",
    "program = Program(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Submmited Model\n",
      "[*] - Initializing Classifier\n"
     ]
    }
   ],
   "source": [
    "# Initialize submitted model\n",
    "program.initialize_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Calling fit method of submitted model\n",
      "[*] - Training Classifier on the train set\n"
     ]
    }
   ],
   "source": [
    "# Call fit method of submitted model\n",
    "program.fit_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Calling predict method of submitted model\n",
      "[*] - Predicting test set using trained Classifier\n"
     ]
    }
   ],
   "source": [
    "# Call predict method of submitted model\n",
    "program.predict_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Score\n",
    "***\n",
    "\n",
    "TODOs:\n",
    "- Explain the evaluation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score():\n",
    "\n",
    "    def __init__(self, data, program):\n",
    "\n",
    "        self.data = data\n",
    "        self.program = program\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Score\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "    def compute_scores(self):\n",
    "        print(\"[*] Computing scores\")\n",
    "\n",
    "        _, y_test = self.data.get_test_data()\n",
    "        y_test_hat = self.program.y_test_hat\n",
    "\n",
    "        # Classification report\n",
    "        print(classification_report(y_test, y_test_hat))\n",
    "\n",
    "        k_tau, _ = kendalltau(y_test, y_test_hat)\n",
    "        print(f\"Kendall's Tau: {k_tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Score\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize Score\n",
    "score = Score(data=data, program=program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Computing scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.52      0.50        96\n",
      "           1       0.55      0.49      0.52       107\n",
      "           2       0.57      0.59      0.58        91\n",
      "           3       0.75      0.75      0.75       106\n",
      "\n",
      "    accuracy                           0.59       400\n",
      "   macro avg       0.59      0.59      0.59       400\n",
      "weighted avg       0.59      0.59      0.59       400\n",
      "\n",
      "Kendall's Tau: 0.6782257738683362\n"
     ]
    }
   ],
   "source": [
    "# Compute Score\n",
    "score.compute_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Submissions\n",
    "***\n",
    "\n",
    "### **Unit Testing**\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission.\n",
    "<br>\n",
    "Keep the sample code simple.<br>\n",
    "\n",
    "<code>python3</code> is required for this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Ingestion Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "### Ingestion Program\n",
      "############################################\n",
      "\n",
      "[*] Loading Data\n",
      "[*] Transforming Data\n",
      "[*] Prepare Data for Training\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "[*] Initializing Submmited Model\n",
      "[*] - Initializing Classifier\n",
      "[*] Calling fit method of submitted model\n",
      "[*] - Training Classifier on the train set\n",
      "[*] Calling predict method of submitted model\n",
      "[*] - Predicting test set using trained Classifier\n",
      "[*] Saving ingestion result\n",
      "\n",
      "---------------------------------\n",
      "[✔] Total duration: 0:05:03.166551\n",
      "---------------------------------\n",
      "\n",
      "----------------------------------------------\n",
      "[✔] Ingestions Program executed successfully!\n",
      "----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 $program_dir/ingestion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Scoring Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "### Scoring Program\n",
      "############################################\n",
      "\n",
      "[*] Reading predictions\n",
      "[✔]\n",
      "[*] Computing scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.56      0.53        96\n",
      "           1       0.60      0.50      0.55       107\n",
      "           2       0.58      0.63      0.60        91\n",
      "           3       0.77      0.75      0.76       106\n",
      "\n",
      "    accuracy                           0.61       400\n",
      "   macro avg       0.61      0.61      0.61       400\n",
      "weighted avg       0.61      0.61      0.61       400\n",
      "\n",
      "Kendall's Tau: 0.6919507236667067\n",
      "[✔]\n",
      "[*] Writing scores\n",
      "[✔]\n",
      "\n",
      "---------------------------------\n",
      "[✔] Total duration: 0:00:00.006382\n",
      "---------------------------------\n",
      "\n",
      "----------------------------------------------\n",
      "[✔] Scoring Program executed successfully!\n",
      "----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 $score_dir/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the submission**\n",
    "\n",
    "TODOs:  \n",
    "- The following submission will be submitted by the participants to your competition website. Describe this clearly and point to the competition once your website is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit : Relevance-code_submission_24-01-12-00-27.zip to the competition\n",
      "You can find the zip file in `M1-Challenge-Class-2024/Relevance/Starting_Kit/\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "code_submission = 'Relevance-code_submission_' + the_date + '.zip'\n",
    "zipdir(code_submission, submission_dir)\n",
    "print(\"Submit : \" + code_submission + \" to the competition\")\n",
    "print(\"You can find the zip file in `M1-Challenge-Class-2024/Relevance/Starting_Kit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9e001b0608738f9411416229c98988c04b997dc526fb61c5e4e084e768e3249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
